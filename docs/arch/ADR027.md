# ADR027 - TITLE

- **Status**: Draft
- **Date**: 2025-02-19
- **Author**: Joshua Taylor MBCS

## Decision

<!-- 
    In a few sentences, describe the decision taken. 
-->

## Context

In July 2024, the product manager made a feature request to provide end-user metric
and usage reports for the family hubs services for consumption by LA admins,
VCFS admins and DfE admins.

These reports would need to contain, for example:

- the number of organisations recorded
- the number of services stored
- how many searches have been queried

These reports needed be accessible through the 'Manage' service. An
approach for how the reports could be generated and how their underlying data
could be collected needed to be decided.

The approach needed to satisfy the (at the time) high scalability requirements
of the 'Find' and 'Connect' services. Accessing the reports needed to also not
exceed acceptance performance ranges.

## Options considered

<!-- 
    Briefly describe each option considered as a numbered list. Start with the selected option.
    It's usually wise to include a 'do nothing' option.

    e.g.

    1. (SELECTED) PostgreSQL
    2. Oracle
    3. SQL Server  
-->

## Consequences

### Option 1 - Azure data factory ETL pipeline to generate service-fronted data warehouse

- Acessing metrics would be very performant.

- If high user load appears, there would be no degredation of performance.
  Scales well.

- Quite complex to implement, requires multiple new components which bring new
  maintenance and infrastructure costs.

- An Azure data factory (ADF) pipeline is hard to manage using the team's
  current infrastructure as code, increasing the overhead of creating new environments.

- Usage of a 'star schema' data warehouse would make any new metric requirements
  easy to provide, provided the right data is loaded.

- ADF pipelines are hard to change and are error-prone if implemented with
  steps that rely on the underlying data schema, such as stored procedures.

- One of the reasons to use ADF is it's ability to graphically create pipelines,
  which would not be used in the implementation of this approach.

### Option 2 - Directly query any service-related metrics

- No new components would be required, just added features to existing
  components.

- If user load grows, the execution time of queries would also grow. Could use
  caching to mitigate this.

- The resulting solution would be easier to change and extend.

## Advice

- Aaron Yarborough, Lead developer

    - A separate data warehouse would separate the read load from reporting and
      the write operation from the recording of metrics. Would reduce any
      potential row-locking delays at high volumes.

    - Management of ADF pipelines is possible with IaC, just difficult to do
      with our chosen IaC tool (terraform).

    - With a data warehouse, we'd still need to modify all components in the
      chain to support a new metric.

    - The use of stored procedures in ADF makes it more expensive and
      error-prone to support new reporting requirements. 

- Zac King, Developer

    - ADF is used wrong. We don't use any of its graphical capabilities and the
      use of stored procedures makes it hard to change. It's essentially acting
      like an Azure function.

    - An ETL approach is asynchronous and fast, but that might not be completely
      suitable for this project.

    - The direct query approach would be much more straightforward to develop,
      but potentially less scalable.

    - A read-replica could be used to mitigate performance concerns with the
      direct query approach.

    - Development at some point got paused due to the effort required to add new
      metrics with the ETL approach.

